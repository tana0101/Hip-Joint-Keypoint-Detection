# ğŸ¦´åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é«–é—œç¯€é—œéµé»åµæ¸¬ç³»çµ±

<div align="center">
  <div>
    <a href="https://github.com/tana0101/Hip-Joint-Keypoint-Detection/README_zh_TW.md">ğŸ‡¹ğŸ‡¼ç¹é«”ä¸­æ–‡</a> |
    <a href="https://github.com/tana0101/Hip-Joint-Keypoint-Detection/README.md">ğŸŒEnglish</a> |
    <a href="https://deepwiki.com/tana0101/Hip-Joint-Keypoint-Detection">ğŸ“šDeepWiki</a> |
    <a href="https://github.com/tana0101/Hip-Joint-Keypoint-Detection/issues">â“issues</a><!-- |
    ğŸ“Paper(å°šæœªç™¼è¡¨)-->
  </div>
<br>
  <img src="src/img/project_banner.png" style="width: 70%;"/>
<br>
    <a href="https://app.codacy.com/gh/tana0101/Hip-Joint-Keypoint-Detection/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade"><img src="https://app.codacy.com/project/badge/Grade/800c026fb9d1418e9cb735d1455c3383"/></a>
    <img alt="Using Python version" src="https://img.shields.io/badge/python-3.10-blue.svg">
    <img alt="GitHub last commit" src="https://img.shields.io/github/last-commit/tana0101/Hip-Joint-Keypoint-Detection">
    <img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white"/>
    <img alt="Ultralytics YOLO" src="https://img.shields.io/badge/Ultralytics%20YOLO-%23000000.svg?style=flat&logo=ultralytics&logoColor=white"/>
</div>

## ğŸ“‹Overview

æœ¬å°ˆæ¡ˆæå‡ºä¸€å¥—**åŸºæ–¼æ·±åº¦å­¸ç¿’çš„é«–é—œç¯€é—œéµé»åµæ¸¬ç³»çµ±**ï¼Œç›®æ¨™ç‚ºè¼”åŠ©å°å…’é«–é—œç¯€ç™¼è‚²ä¸è‰¯ï¼ˆDevelopmental Dysplasia of the Hip, **DDH**ï¼‰ä¹‹é‡æ¸¬èˆ‡åˆ†ç´šï¼š  
(1) è‡ªå‹•åµæ¸¬é«–é—œç¯€é—œéµé»ï¼Œ(2) è¨ˆç®— **Acetabular Index (AI) angle**ï¼Œ(3) é€²è¡Œ **IHDI åˆ†é¡**ã€‚  
ç³»çµ±æ¡ç”¨**ç”±ä¸Šè€Œä¸‹ï¼ˆTop-downï¼‰çš„å…©éšæ®µæµç¨‹**ï¼šå…ˆä»¥ **YOLO** åµæ¸¬/è£åˆ‡é«–é—œç¯€å€åŸŸï¼Œå†ä»¥é—œéµé»æ¨¡å‹é€²è¡Œå–®å´ï¼ˆleft/rightï¼‰é—œéµé»åµæ¸¬ã€‚

> **Research-only Notice**ï¼šæœ¬å°ˆæ¡ˆç‚ºè³‡è¨Šå·¥ç¨‹ç ”ç©¶ç”¨é€”ä¹‹åŸå‹ç³»çµ±ï¼Œè¼¸å‡ºçµæœä¸å¾—ç›´æ¥ä½œç‚ºè‡¨åºŠè¨ºæ–·ä¾æ“šã€‚

## ğŸ“‘Table of Contents

- [Introduction](#introduction)
- [Key Features](#key-features)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Installation](#installation)
- [Usage (One-fold)](#usage-onefold-training--evaluation)
- [Usage (K-Fold)](#usage-k-fold-cross-validation)
- [Results](#results)
- [References](#references)

## Introduction

å°å…’é«–é—œç¯€ç™¼è‚²ä¸è‰¯ï¼ˆDDHï¼‰æ˜¯ä¸€ç¨®å¸¸è¦‹ä½†å®¹æ˜“è¢«å¿½ç•¥çš„éª¨éª¼ç™¼è‚²ç–¾ç—…ï¼Œè‹¥æœªèƒ½åŠæ—©è¨ºæ–·èˆ‡ä»‹å…¥ï¼Œå¯èƒ½å°å­©ç«¥æœªä¾†çš„è¡Œèµ°èƒ½åŠ›èˆ‡éª¨éª¼ç™¼è‚²é€ æˆé•·æœŸå½±éŸ¿ã€‚

åœ¨è‡¨åºŠå¯¦å‹™ä¸Šï¼ŒDDH çš„è¨ºæ–·é«˜åº¦ä»°è³´é†«å¸«å° X å…‰å½±åƒçš„äººå·¥åˆ¤è®€èˆ‡é‡æ¸¬ï¼Œéç¨‹å…·æœ‰ä¸€å®šçš„ä¸»è§€æ€§ï¼Œä¸”åœ¨ä¸åŒé†«å¸«æˆ–ä¸åŒæ™‚é–“é»ä¹‹é–“ï¼Œå®¹æ˜“ç”¢ç”Ÿé‡æ¸¬å·®ç•°ã€‚

è¿‘å¹´ä¾†ï¼Œæ·±åº¦å­¸ç¿’åœ¨é†«å­¸å½±åƒåˆ†æé ˜åŸŸå±•ç¾å‡ºå“è¶Šçš„è¡¨ç¾ï¼Œç‰¹åˆ¥é©åˆæ‡‰ç”¨æ–¼é—œéµé»åµæ¸¬ã€è§’åº¦é‡æ¸¬èˆ‡ç–¾ç—…åˆ†ç´šç­‰ä»»å‹™ã€‚æœ¬å°ˆæ¡ˆå³åˆ©ç”¨æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œè‡ªå‹•åµæ¸¬é«–é—œç¯€é—œéµé»ï¼Œé€²ä¸€æ­¥è¼”åŠ©è‡¨åºŠé€²è¡Œ DDH ç›¸é—œæŒ‡æ¨™çš„è¨ˆç®—èˆ‡åˆ†é¡ã€‚

## âœ¨Key Features

- ğŸ“ **é«–é—œç¯€é—œéµé»åµæ¸¬**ï¼šè‡ªå‹•é æ¸¬é«–é—œç¯€é—œéµé»åº§æ¨™ã€‚
- ğŸ“ **è‡¨åºŠæŒ‡æ¨™é‡æ¸¬**ï¼šæ”¯æ´ Acetabular Index (AI) Angle è¨ˆç®—èˆ‡ IHDI åˆ†é¡ã€‚
- ğŸ—ï¸ **å¤šæ¨¡å‹æ¶æ§‹æ”¯æ´**ï¼šå¯å½ˆæ€§é¸æ“‡ä¸åŒ Backboneï¼ˆå¦‚ ConvNeXtã€HRNetã€EfficientNet ç­‰ï¼‰ã€‚
- ğŸ¯ **å¤šç¨® Head è¨­è¨ˆ**ï¼šæ”¯æ´ Direct Regression èˆ‡ SimCC ç³»åˆ—ç­‰ä¸åŒé—œéµé»é æ¸¬ç­–ç•¥ã€‚
- ğŸ§© **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šæ–¹ä¾¿é€²è¡Œæ¨¡å‹æ›¿æ›ã€å¯¦é©—æ¯”è¼ƒèˆ‡æ“´å……ç ”ç©¶ã€‚

## ğŸ’¾Dataset

- è³‡æ–™å­˜æ”¾æ–¼ `dataset/` ç›®éŒ„ä¸­ã€‚
- æ¨™è¨»ç¨‹å¼ä½æ–¼ `Keypoint-Annotation-Tool/` ç›®éŒ„ä¸­ã€‚
ğŸš§ **è³‡æ–™é›†ä¸‹è¼‰èˆ‡æ•´ç†æµç¨‹å°‡æ–¼å¾ŒçºŒè£œå……** ğŸš§

### ğŸ¥xray_IHDIï¼ˆä¸»è¦å¯¦é©—è³‡æ–™é›†ï¼‰
<img src="src/img/sample_IHDI.jpg" style="width: 30%;"/>

æœ¬ç ”ç©¶æ¡ç”¨å›æº¯æ€§è³‡æ–™ï¼Œæ”¶é›†ä¾†è‡ªæˆå¤§é†«é™¢æ–¼ 2015/01/01 è‡³ 2025/01/19 æœŸé–“ï¼Œ4 æ­²ä»¥ä¸‹å¬°å¹¼å…’ä¹‹é«–éƒ¨ X å…‰å½±åƒã€‚åŸå§‹ç´å…¥ 622 ä»½å½±åƒï¼Œç¶“æ’é™¤ç•°å¸¸å€¼å¾Œä¿ç•™ 557 å¼µã€‚æ¯å¼µå½±åƒç”±è‡¨åºŠå°ˆæ¥­é†«å¸«æ‰‹å‹•æ¨™è¨» **12 å€‹é—œéµé»**ï¼Œä¸¦æä¾› LeftHip / RightHip ç‰©ä»¶æ¨™ç±¤ä¾›åµæ¸¬éšæ®µè¨“ç·´ã€‚

- å½±åƒæ¨™è¨»ï¼šæ¯å¼µå½±åƒå°æ‡‰ä¸€å€‹ .csvï¼Œæ ¼å¼ï¼š
```
"(x1,y1)","(x2,y2)",...,"(x12,y12)"
```
- å‚™è¨»ï¼šåŸºæ–¼é†«ç™‚éš±ç§èˆ‡è³‡æ–™ä¿è­·è¦ç¯„ï¼Œç„¡æ³•å…¬é–‹é‡‹å‡ºã€‚

<hr>

### ğŸŒMTDDHï¼ˆå…¬é–‹è³‡æ–™é›†ï¼‰
<img src="src/img/sample_MTDDH.jpg" style="width: 30%;"/>

- è³‡æ–™ä¾†æºï¼š[open-hip-dysplasia](https://github.com/radoss-org/open-hip-dysplasia.git)
- è³‡æ–™é‡ï¼š1751 å¼µé«–é—œç¯€ X å…‰å½±åƒï¼ˆå·²æ’é™¤ç•°å¸¸å€¼ï¼‰
- æ¨™è¨»å…§å®¹ï¼š
  - **8 å€‹é—œéµé»**
  - LeftHip / RightHip ç‰©ä»¶æ¨™ç±¤

<hr>

### ğŸ“Šè³‡æ–™çµ±è¨ˆåˆ†ä½ˆ

**Acetabular Index (AI) åˆ†ä½ˆ**
<img src="dataset/xray_IHDI_AI_Distribution.png" />
<img src="dataset/mtddh_xray_2d_AI_Distribution.png" />

**IHDI åˆ†é¡åˆ†ä½ˆ**
<div style="display: flex; justify-content: space-between; gap: 10px;">
  <img src="dataset/xray_IHDI_IHDI_Distribution.png" style="width: 49%;" />
  <img src="dataset/mtddh_xray_2d_IHDI_Distribution.png" style="width: 49%;" />
</div>

## ğŸ› ï¸Methodology

æœ¬å°ˆæ¡ˆæ¡ç”¨ç”±ä¸Šè€Œä¸‹ï¼ˆTop-downï¼‰çš„å…©éšæ®µé—œéµé»åµæ¸¬æµç¨‹ï¼š
1. **ğŸ” ç‰©ä»¶åµæ¸¬èˆ‡å–®é‚Šè£åˆ‡**ï¼šä»¥ YOLO åµæ¸¬ LeftHip / RightHipï¼Œè£åˆ‡ ROIï¼ˆé™ä½èƒŒæ™¯å¹²æ“¾ï¼‰ã€‚
2. **ğŸ§  å–®å´é—œéµé»åµæ¸¬**ï¼šå°è£åˆ‡å¾Œå–®å´ hip ROI é€²è¡Œé—œéµé»åµæ¸¬ï¼ˆå¤š Backbone / Head ç ”ç©¶æ¯”è¼ƒï¼‰ã€‚

### Head Architecture

<img src="src/img/head_design.png" style="width: 99%;" />

æœ¬å°ˆæ¡ˆæ”¯æ´å¤šç¨®é—œéµé» Head è¨­è¨ˆï¼Œä»¥å› æ‡‰ä¸åŒæ¨¡å‹ç‰¹æ€§èˆ‡å¯¦é©—éœ€æ±‚ï¼š

- **SimCC 2D / SimCC 2D Deconv**ï¼š
å®˜æ–¹ SimCC ç³»åˆ—ï¼Œå°‡åº§æ¨™å›æ­¸è½‰ç‚º x/y ä¸€ç¶­åˆ†é¡åˆ†ä½ˆï¼Œé€é soft-argmax å–å¾—åº§æ¨™ã€‚
- **SimCC 1Dï¼ˆè‡ªè¨‚è®Šé«”ï¼‰**ï¼š
ä»¥ Global Average Pooling å£“ç¸®ç‰¹å¾µåœ–ï¼Œä½¿ç”¨å…¨é€£æ¥å±¤é æ¸¬ x/y åˆ†ä½ˆä»¥é™ä½è¤‡é›œåº¦ã€‚
- **Direct Regression**ï¼š
ä»¥å…¨é€£æ¥å±¤ç›´æ¥å›æ­¸ (x, y) åº§æ¨™ã€‚

### Backbone Architecture

ç›®å‰æ”¯æ´çš„ Backbone æ¶æ§‹å¦‚ä¸‹ï¼š

- ConvNeXtV1  
  - `ConvNeXtSmallCustom`
- ConvNeXtV1 + Feature Pyramid Networkï¼ˆå¤šå°ºåº¦ç‰¹å¾µï¼‰  
  - `ConvNeXtSmallMS`
- HRNet  
  - `HRNetW32Custom`
  - `HRNetW48Custom`

å…¶ä¸­ `Custom` çµå°¾ä»£è¡¨æœ¬å°ˆæ¡ˆåŸºæ–¼å®˜æ–¹å¯¦ä½œé€²è¡Œä¿®æ”¹èˆ‡å„ªåŒ–ï¼Œä»¥æ›´ç¬¦åˆé«–é—œç¯€é—œéµé»åµæ¸¬ä»»å‹™çš„éœ€æ±‚ã€‚

ğŸš§ **å…¶ä»– Backboneï¼ˆå¦‚ EfficientNetã€InceptionNeXt ç­‰ï¼‰ç›®å‰ä»åœ¨é–‹ç™¼èˆ‡æ¸¬è©¦ä¸­ï¼Œä»¥ç¢ºä¿å¯èˆ‡ä¸åŒ Head æ¶æ§‹ç›¸å®¹** ğŸš§

### Other Techniques

- **ğŸ”„ Data Augmentation**ï¼šRandom Rotation / Random Translation
- **ğŸ“‰ Loss Functions**
  - **Direct Regression**: MSE Loss
    $$
    L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} \left( (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 \right)
    $$
  - **SimCC Series**: KL Divergence Loss
    $$
    L_{KL} = \frac{1}{N} \sum_{i=1}^{N} \left( D_{KL}(P_{x_i} || \hat{P}_{x_i}) + D_{KL}(P_{y_i} || \hat{P}_{y_i}) \right)
    $$
- **âš™ï¸ Optimizers**ï¼šAdamW
- **ğŸ“ˆ LR Schedulers**ï¼šCosine Annealing + Warmup

## ğŸ“‚Project Structure

```text
Hip-Joint-Keypoint-Detection/
â”œâ”€â”€ dataset/                        # ğŸ’¾ è³‡æ–™é›†å­˜æ”¾ç›®éŒ„
â”‚   â”œâ”€â”€ xray_IHDI/                  # ä¸»è¦å¯¦é©—è³‡æ–™é›† (Private)
â”‚   â””â”€â”€ mtddh/                      # å…¬é–‹è³‡æ–™é›† (Public)
â”œâ”€â”€ datasets/                       # è³‡æ–™é›†è¼‰å…¥èˆ‡è™•ç†æ¨¡çµ„
â”œâ”€â”€ models/                         # ğŸ§  æ¨¡å‹èˆ‡ Head çš„å®šç¾©èˆ‡å¯¦ä½œ
â”œâ”€â”€ src/                            # ç³»çµ±æ ¸å¿ƒè³‡æºèˆ‡åœ–ç‰‡
â”‚   â””â”€â”€ img/                        # README ä½¿ç”¨ä¹‹å±•ç¤ºåœ–ç‰‡
â”œâ”€â”€ utils/                          # ğŸ› ï¸ é€šç”¨å·¥å…·å‡½å¼åº«
â”œâ”€â”€ weights/                        # ğŸ“¥ è¨“ç·´å®Œæˆçš„æ¨¡å‹æ¬Šé‡å­˜æ”¾å€ (.pth)
â”œâ”€â”€ logs/                           # ğŸ“ è¨“ç·´éç¨‹æ—¥èªŒèˆ‡æå¤±æ›²ç·šåœ–
â”œâ”€â”€ results/                        # ğŸ“Š çµ±è¨ˆçµæœè¼¸å‡ºç›®éŒ„
â”œâ”€â”€ Keypoint-Annotation-Tool/       # ğŸ–Šï¸ é—œéµé»æ¨™è¨»å·¥å…·
â”œâ”€â”€ Hip-Joint-Keypoint-Detection-Tool/ # ğŸ–¥ï¸ è‡¨åºŠè¼”åŠ©ä»‹é¢åŸå‹ (WIP)
â”œâ”€â”€ train_yolo.py                   # [è¨“ç·´] YOLO ç‰©ä»¶åµæ¸¬æ¨¡å‹
â”œâ”€â”€ train_hip_crop_keypoints.py     # [è¨“ç·´] å–®å´é«–é—œç¯€é—œéµé»æ¨¡å‹
â”œâ”€â”€ predict_hip_crop_keypoints.py   # [æ¨è«–] åŸ·è¡Œå®Œæ•´çš„åµæ¸¬èˆ‡è©•ä¼°
â”œâ”€â”€ split.py                        # [å·¥å…·] è³‡æ–™é›†åˆ†å‰² (Train/Val/Test)
â”œâ”€â”€ kfold_split.py                  # [å·¥å…·] K-Fold è³‡æ–™é›†åˆ†å‰²
â”œâ”€â”€ kfold_train_yolo.py             # [K-Fold] YOLO äº¤å‰é©—è­‰è¨“ç·´
â”œâ”€â”€ kfold_train_hip_crop_keypoints.py # [K-Fold] é—œéµé»äº¤å‰é©—è­‰è¨“ç·´
â”œâ”€â”€ kfold_predict_hip_crop_keypoints.py # [K-Fold] äº¤å‰é©—è­‰è©•ä¼°
â”œâ”€â”€ requirements.txt                # ğŸ“¦ å°ˆæ¡ˆä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ README.md                       # ğŸ‡¬ğŸ‡§ è‹±æ–‡èªªæ˜æ–‡ä»¶
â””â”€â”€ README_zh_TW.md                 # ğŸ‡¹ğŸ‡¼ ç¹é«”ä¸­æ–‡èªªæ˜æ–‡ä»¶
```

## ğŸ“¦Installation

æŒ‰ç…§ä»¥ä¸‹æ–¹å¼å®‰è£æ‰€éœ€çš„ Python ç’°å¢ƒèˆ‡å¥—ä»¶ï¼š

### ä½¿ç”¨ Conda å‰µå»ºè™›æ“¬ç’°å¢ƒ

1. è¤‡è£½å°ˆæ¡ˆï¼š
```
   git clone https://github.com/tana0101/Hip-Joint-Keypoint-Detection.git
   cd Hip-Joint-Keypoint-Detection
```
2. å‰µå»ºä¸¦å•Ÿå‹• Conda è™›æ“¬ç’°å¢ƒï¼š
```
   conda create -n hip_joint_detection python=3.10
   conda activate hip_joint_detection
```
3. å®‰è£æ‰€éœ€å¥—ä»¶ï¼š
```
   pip install -r requirements.txt
```

### ä½¿ç”¨ä¸€èˆ¬ Python ç’°å¢ƒ

1. è¤‡è£½å°ˆæ¡ˆï¼š
```
   git clone https://github.com/tana0101/Hip-Joint-Keypoint-Detection.git
   cd Hip-Joint-Keypoint-Detection
```
2. å®‰è£æ‰€éœ€å¥—ä»¶ï¼š
```
   pip install -r requirements.txt
```

## ğŸš€Usage (One-fold): Training & Evaluation

### Split Dataset

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: split.py [-h] --dataset DATASET [--out OUT] [--train TRAIN] [--val VAL]
                [--test TEST] [--seed SEED]

Split dataset into train/val/test with multiple modalities and emit
Ultralytics data.yaml

options:
  -h, --help         show this help message and exit
  --dataset DATASET  Root directory of the dataset, e.g., dataset/xray_IHDI
  --out OUT          Output root directory (default: data)
  --train TRAIN      Train split ratio (default: 0.8)
  --val VAL          Validation split ratio (default: 0.1)
  --test TEST        Test split ratio (default: 0.1)
  --seed SEED        Random seed (default: 42)
```
</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python split.py --dataset dataset/xray_IHDI --out data --train 0.8 --val 0.1 --test 0.1 --seed 42
```

å°‡æœƒåˆ†å‰²å‡º `data/train`ã€`data/val`ã€`data/test` ä¸‰å€‹å­ç›®éŒ„ï¼Œä¸¦ç”¢ç”Ÿç‰©ä»¶åµæ¸¬ç”¨çš„ `data/data.yaml` æª”æ¡ˆä¾›å¾ŒçºŒè¨“ç·´ä½¿ç”¨ã€‚

### Training

ç”±æ–¼æœ¬å°ˆæ¡ˆæ¡ç”¨å…©éšæ®µï¼Œå…ˆä½¿ç”¨ YOLO åµæ¸¬ä¸¦è£åˆ‡å‡ºé«–é—œç¯€å€åŸŸï¼Œå†é€²è¡Œå–®é‚Šé—œéµé»åµæ¸¬æ¨¡å‹çš„è¨“ç·´ã€‚

#### Step 1: Train YOLO Detector

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python train_yolo.py \
  --model yolo12s.pt \
  --data data/data.yaml \
  --epochs 300 --imgsz 640 --batch 8 --device 0 \
  --project runs/train --name yolo12s --pretrained --seed 42 \
  --fliplr 0.0 --flipud 0.0 --degrees 5.0 \
  --shear 0.0 --perspective 0.0 --mosaic 0.0 --mixup 0.0
```

âš ï¸æ³¨æ„ï¼šè¨“ç·´å®Œæˆå¾Œï¼Œè«‹å°‡æœ€ä½³æ¬Šé‡ `(runs/train/exp_name/weights/best.pt)` ç§»å‹•ä¸¦é‡æ–°å‘½åè‡³ `weights/` è³‡æ–™å¤¾ï¼ˆä¾‹å¦‚å‘½åç‚º `yolo12s.pt`ï¼‰ï¼Œä»¥é…åˆå¾ŒçºŒæ¨è«–æ­¥é©Ÿã€‚

#### Step 2: Train Keypoint Detector

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: train_hip_crop_keypoints.py [-h] --data_dir DATA_DIR --model_name
                                   MODEL_NAME [--epochs EPOCHS]
                                   [--input_size INPUT_SIZE]
                                   [--learning_rate LEARNING_RATE]
                                   [--batch_size BATCH_SIZE]
                                   [--side {left,right}] [--mirror]
                                   [--head_type {direct_regression,simcc_1d,simcc_2d,simcc_2d_deconv}]
                                   [--split_ratio SPLIT_RATIO] [--sigma SIGMA]

options:
  -h, --help            show this help message and exit
  --data_dir DATA_DIR   Path to the dataset directory
  --model_name MODEL_NAME
                        Model name: 'efficientnet', 'resnet', or 'vgg'
  --epochs EPOCHS       Number of training epochs
  --input_size INPUT_SIZE
                        Input image size for the model
  --learning_rate LEARNING_RATE
                        Learning rate
  --batch_size BATCH_SIZE
                        Number of samples per batch
  --side {left,right}   Side to train on: 'left' or 'right'
  --mirror              Whether to include mirrored data from the opposite
                        side
  --head_type {direct_regression,simcc_1d,simcc_2d,simcc_2d_deconv}
                        Type of model head to use
  --split_ratio SPLIT_RATIO
                        SimCC split ratio for label encoding
  --sigma SIGMA         Sigma for SimCC label encoding
```

</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python3 train_hip_crop_keypoints.py --data_dir data --model_name convnext_small_custom --input_size 224 --epochs 200 --learning_rate 0.0001 --batch_size 32 --side left --mirror --head_type simcc_2d --split_ratio 3.0 --sigma 7.0
```

è¨“ç·´çµæœï¼š
- æœ€ä½³æ¨¡å‹æœƒå­˜ç‚ºï¼š
```
weights/{model_name}_{head_type}_{split_ratio}_{sigma}_{side}_{mirror}_{input_size}_{epochs}_{learning_rate}_{batch_size}_best.pth
```
ç¯„ä¾‹ï¼šconvnext_small_custom_simcc_2d_sr3.0_sigma7.0_cropleft_mirror_224_200_0.0001_32_best.pth 
- è¨“ç·´éç¨‹åœ–è¡¨æœƒå­˜ç‚ºï¼š
```
logs/{model_name}_{head_type}_{split_ratio}_{sigma}_{side}_{mirror}_{input_size}_{epochs}_{learning_rate}_{batch_size}_training_plot.png
```
ç¯„ä¾‹ï¼šconvnext_small_custom_simcc_2d_sr3.0_sigma7.0_cropleft_mirror_224_200_0.0001_32_training_plot.png
- è¨“ç·´æ—¥èªŒæœƒå­˜ç‚ºï¼š
```
logs/{model_name}_{head_type}_{split_ratio}_{sigma}_{side}_{mirror}_{input_size}_{epochs}_{learning_rate}_{batch_size}_training_log.txt
```
ç¯„ä¾‹ï¼šconvnext_small_custom_simcc_2d_sr3.0_sigma7.0_cropleft_mirror_224_200_0.0001_32_training_log.txt

### Evaluation

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: predict_hip_crop_keypoints.py [-h] --model_name MODEL_NAME
                                     [--kp_left_path KP_LEFT_PATH]
                                     [--kp_right_path KP_RIGHT_PATH]
                                     --yolo_weights YOLO_WEIGHTS --data DATA
                                     [--output_dir OUTPUT_DIR]
                                     [--fold_index FOLD_INDEX]

options:
  -h, --help            show this help message and exit
  --model_name MODEL_NAME
                        efficientnet | resnet | vgg
  --kp_left_path KP_LEFT_PATH
                        left-side KP model (.pth)
  --kp_right_path KP_RIGHT_PATH
                        right-side KP model (.pth)
  --yolo_weights YOLO_WEIGHTS
                        YOLO weights (e.g., best.pt)
  --data DATA           data directory
  --output_dir OUTPUT_DIR
                        output directory
  --fold_index FOLD_INDEX
                        fold index for k-fold cross-validation (optional)
```

</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python3 predict_hip_crop_keypoints.py --model_name convnext_small_custom --kp_left_path weights/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_cropleft_mirror_224_200_0.0001_32_best.pth --yolo_weights models/yolo12s.pt --data "data/test" --output_dir "results"
```

çµ±è¨ˆçµæœæœƒå­˜æ–¼ `{output_dir}` ç›®éŒ„ä¸­ã€‚
âš ï¸æ³¨æ„ï¼šè«‹ç¢ºä¿ç‰©ä»¶åµæ¸¬æ¬Šé‡ï¼ˆyolo_weightsï¼‰èˆ‡é—œéµé»æ¬Šé‡ï¼ˆkp_pathï¼‰çš†å·²è¨“ç·´å®Œæˆä¸¦å­˜åœ¨å°æ‡‰è·¯å¾‘ã€‚

## ğŸ”„Usage (K-Fold Cross Validation)

### Split Dataset

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: kfold_split.py [-h] --src SRC --dst DST [--k K] [--seed SEED]
                      [--overwrite]

K-fold split for object detection and keypoint datasets

options:
  -h, --help   show this help message and exit
  --src SRC    Source directory containing images/, annotations/, detections/, yolo_labels/
  --dst DST    Destination directory where fold1..foldK and data_fold{i}.yaml will be created
  --k K        Number of folds
  --seed SEED  Random seed for shuffling
  --overwrite  If the destination directory exists, delete it before creating new folds
```

</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python kfold_split.py \
  --src dataset/xray_IHDI \
  --dst data \
  --k 5 \
  --seed 42 \
  --overwrite
```

### Training

#### Step 1: Train YOLO Detector

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python kfold_train_yolo.py \
  --model yolo12s.pt \
  --data_tpl data/data_fold{fold}.yaml \
  --k 5 \
  --epochs 300 --imgsz 640 --batch 8 --device 0 \
  --project runs/train --name yolo12s_kfold --pretrained --seed 42 \
  --fliplr 0.0 --flipud 0.0 --degrees 5.0 \
  --shear 0.0 --perspective 0.0 --mosaic 0.0 --mixup 0.0
```
âš ï¸æ³¨æ„ï¼šè¨“ç·´å®Œæˆå¾Œï¼Œè«‹å°‡å„ Fold çš„æœ€ä½³æ¬Šé‡ `(runs/train/exp_name/weights/best.pt)` ç§»å‹•ä¸¦é‡æ–°å‘½åè‡³ `weights/` è³‡æ–™å¤¾ï¼ˆä¾‹å¦‚å‘½åç‚º `yolo12s_fold{i}.pt`ï¼‰ï¼Œä»¥é…åˆå¾ŒçºŒæ¨è«–æ­¥é©Ÿã€‚

#### Step 2: Train Keypoint Detector

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: kfold_train_hip_crop_keypoints.py [-h] --data_root DATA_ROOT [--k K]
                                         [--mode {outer_inner,val_as_test}]
                                         [--inner_val_ratio INNER_VAL_RATIO]
                                         [--inner_seed INNER_SEED] [--copy]
                                         --model_name MODEL_NAME
                                         [--epochs EPOCHS]
                                         [--input_size INPUT_SIZE]
                                         [--learning_rate LEARNING_RATE]
                                         [--batch_size BATCH_SIZE]
                                         [--side {left,right}] [--mirror]
                                         [--head_type {direct_regression,simcc_1d,simcc_2d,simcc_2d_deconv}]
                                         [--split_ratio SPLIT_RATIO]
                                         [--sigma SIGMA]

K-fold training for hip crop keypoints.

options:
  -h, --help            show this help message and exit
  --data_root DATA_ROOT
                        åŒ…å« fold1..foldK çš„è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ data æˆ–
                        dataset/xray_IHDI_kfold
  --k K                 fold æ•¸é‡
  --mode {outer_inner,val_as_test}
                        outer_inner = Outer K-fold + Inner split; val_as_test
                        = å–®å±¤ K-foldï¼Œval åŒæ™‚ä¹Ÿæ˜¯ä¹‹å¾Œçš„ test fold
  --inner_val_ratio INNER_VAL_RATIO
                        åœ¨ outer_inner æ¨¡å¼ä¸‹ï¼ŒTrain pool è£¡ç”¨å¤šå°‘æ¯”ä¾‹åš inner validation
  --inner_seed INNER_SEED
                        åœ¨ outer_inner æ¨¡å¼ä¸‹ï¼Œinner split çš„äº‚æ•¸ç¨®å­
  --copy                é è¨­ç”¨ symlink å»º tmp/train,valï¼›åŠ é€™å€‹å°±æ”¹æˆå¯¦éš›è¤‡è£½æª”æ¡ˆï¼ˆè¼ƒè€—ç©ºé–“ï¼‰
  --model_name MODEL_NAME
                        Model name: 'efficientnet', 'resnet', 'vgg',
                        'convnext' ç­‰
  --epochs EPOCHS
  --input_size INPUT_SIZE
  --learning_rate LEARNING_RATE
  --batch_size BATCH_SIZE
  --side {left,right}
  --mirror
  --head_type {direct_regression,simcc_1d,simcc_2d,simcc_2d_deconv}
  --split_ratio SPLIT_RATIO
                        SimCC çš„ sr åƒæ•¸ï¼Œç”¨åœ¨ head_type ç‚º simcc* æ™‚
  --sigma SIGMA         SimCC label encoding çš„ sigma
```

</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python kfold_train_hip_crop_keypoints.py \
  --data_root data \
  --k 5 \
  --mode outer_inner \
  --inner_val_ratio 0.1 \
  --inner_seed 42 \
  --model_name convnext_small_custom \
  --input_size 224 \
  --epochs 200 \
  --learning_rate 0.0001 \
  --batch_size 16 \
  --side left \
  --mirror \
  --head_type simcc_2d \
  --split_ratio 3.0 \
  --sigma 7.0
```

è¨“ç·´çµæœæœƒå­˜æ–¼ `weights/` ç›®éŒ„ä¸­ï¼Œæª”åæ ¼å¼å¦‚ä¸‹ï¼š
```
{model_name}_{head_type}_sr{split_ratio}_sigma{sigma}_crop{side}_{mirror}_{input_size}_{epochs}_{learning_rate}_{batch_size}_fold{fold}_best.pth
```

âš ï¸æ³¨æ„ï¼šè«‹ç¢ºä¿ data_root ç›®éŒ„ä¸­å·²åŒ…å« k-fold åˆ†å‰²å¾Œçš„è³‡æ–™å¤¾ï¼ˆfold1..foldKï¼‰èˆ‡å°æ‡‰çš„ data_fold{i}.yaml æª”æ¡ˆã€‚

### Evaluation

<details><summary><b>é»æ“Šå±•é–‹æŒ‡ä»¤èªªæ˜</b></summary>

```
usage: kfold_predict_hip_crop_keypoints.py [-h] --model_name MODEL_NAME
                                           [--kp_left_tpl KP_LEFT_TPL]
                                           [--kp_right_tpl KP_RIGHT_TPL]
                                           --yolo_weights YOLO_WEIGHTS
                                           --data_root DATA_ROOT [--k K]
                                           [--output_root OUTPUT_ROOT]

K-fold test for hip crop keypoints.

options:
  -h, --help            show this help message and exit
  --model_name MODEL_NAME
                        efficientnet | resnet | vgg | convnext ...
  --kp_left_tpl KP_LEFT_TPL
                        å·¦å´ KP model è·¯å¾‘æ¨¡æ¿ï¼Œä¾‹å¦‚:
                        'weights/convnext_left_fold{fold}_best.pth'ï¼Œæœƒç”¨
                        .format(fold=i) ç”¢ç”Ÿæ¯å€‹ fold çš„è·¯å¾‘ã€‚
  --kp_right_tpl KP_RIGHT_TPL
                        å³å´ KP model è·¯å¾‘æ¨¡æ¿ï¼Œå¯ç•™ç©ºã€‚
  --yolo_weights YOLO_WEIGHTS
                        YOLO weights (e.g., weights/yolo12s_fold{fold}.pt)
  --data_root DATA_ROOT
                        åŒ…å« fold1, fold2, ... çš„è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ data æˆ–
                        dataset/xray_IHDI_kfold
  --k K                 fold æ•¸é‡
  --output_root OUTPUT_ROOT
                        æ¯å€‹ fold çš„è¼¸å‡ºæ ¹ç›®éŒ„ï¼ˆä¸‹å±¤æœƒè‡ªå‹•å»ºç«‹ fold1, fold2, ...ï¼‰
```

</details>

å¸¸ç”¨æŒ‡ä»¤ç¯„ä¾‹ï¼š
```
python kfold_predict_hip_crop_keypoints.py \
  --model_name convnext_small_custom \
  --kp_left_tpl "weights/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_cropleft_mirror_224_200_0.0001_16_fold{fold}_best.pth" \
  --yolo_weights weights/yolo12s_fold{fold}.pt \
  --data_root data \
  --k 5 \
  --output_root results_kfold
```

çµ±è¨ˆçµæœæœƒå­˜æ–¼ `{output_root}/{name}/fold{i}` ç›®éŒ„ä»¥åŠ`{output_root}/{name}/summary` è³‡æ–™å¤¾ä¸­
âš ï¸æ³¨æ„ï¼šè«‹ç¢ºä¿ç‰©ä»¶åµæ¸¬æ¬Šé‡ï¼ˆyolo_weightsï¼‰èˆ‡é—œéµé»æ¬Šé‡ï¼ˆkp_pathï¼‰çš†å·²è¨“ç·´å®Œæˆä¸¦å­˜åœ¨å°æ‡‰è·¯å¾‘ã€‚

## ğŸ†Results

ä¸åŒ Head çš„å¯¦é©—çµæœï¼š
<img src="src/img/experiment.png" style="width: 99%;"/>

ä½¿ç”¨ ConvNeXtSmallCustom + SimCC 2D çš„æ¨¡å‹åœ¨ xray_IHDI è³‡æ–™é›†ä¸Šé€²è¡Œ 5-fold äº¤å‰é©—è­‰çš„çµæœå¦‚ä¸‹ï¼š

<img src="results_kfold/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_left-only_224_200_0.0001_32/summary/all_avg_distances_hist.png" style="width: 49%;"/><img src="results_kfold/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_left-only_224_200_0.0001_32/summary/all_ai_error_hist.png" style="width: 49%;"/>
<img src="results_kfold/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_left-only_224_200_0.0001_32/summary/confusion_matrix_all.png" style="width: 49%;"/><img src="results_kfold/convnext_small_custom_simcc_2d_sr3.0_sigma7.0_left-only_224_200_0.0001_32/summary/scatter_overall_ai_angle.png" style="width: 49%;"/>

## âš¡Inference(WIP)

ğŸš§è‡¨åºŠä½¿ç”¨çš„ä»‹é¢æ­£åœ¨é–‹ç™¼ä¸­ğŸš§
ä½æ–¼ `Hip-Joint-Keypoint-Detection-Tool/` ç›®éŒ„ä¸‹çš„ç¨‹å¼ç¢¼ç‚ºå–®éšæ®µé—œéµé»åµæ¸¬ç³»çµ±çš„åˆæ­¥ä»‹é¢ï¼Œåƒ…ä¾›ç ”ç©¶åƒè€ƒã€‚

## âš ï¸Disclaimer

> **Research Use Only**
> æœ¬ç³»çµ±åƒ…ä¾›å­¸è¡“ç ”ç©¶èˆ‡æŠ€è¡“äº¤æµä½¿ç”¨ï¼Œ**ä¸¦éé†«ç™‚å™¨æ**ã€‚
> æœ¬å°ˆæ¡ˆä¹‹è¼¸å‡ºçµæœä¸å¾—ä½œç‚ºè‡¨åºŠè¨ºæ–·ã€é†«ç™‚æ±ºç­–æˆ–æ²»ç™‚ä¾æ“šã€‚
> æœ¬è»Ÿé«”æŒ‰ã€ŒåŸæ¨£ã€æä¾›ï¼Œä¸æä¾›ä»»ä½•å½¢å¼çš„æ˜ç¤ºæˆ–é»˜ç¤ºæ“”ä¿ï¼ŒåŒ…æ‹¬ä½†ä¸é™æ–¼é©éŠ·æ€§ã€ç‰¹å®šç”¨é€”é©ç”¨æ€§å’Œä¸ä¾µæ¬Šçš„æ“”ä¿ã€‚

## ğŸ“¢Project Note

åŸºæ–¼ç›®å‰çš„ç ”ç©¶é€²åº¦èˆ‡åˆä½œè€ƒé‡ï¼Œæœ¬å„²å­˜åº« (Repository) æš«ä¸å…¬é–‹æ ¸å¿ƒæ¼”ç®—æ³•èˆ‡éƒ¨åˆ†é—œéµæŠ€è¡“ç´°ç¯€ã€‚
è‹¥æ‚¨å°æœ¬å°ˆæ¡ˆçš„å®Œæ•´æŠ€è¡“ç´°ç¯€æœ‰èˆˆè¶£ï¼Œæˆ–æœ‰æ„é€²è¡Œå­¸è¡“/å•†æ¥­åˆä½œï¼Œæ­¡è¿é€é Issue è¨è«–æˆ–ç›´æ¥è¯çµ¡ä½œè€…ã€‚

## âš–ï¸License

æœ¬å°ˆæ¡ˆæ¡ç”¨ **GNU Affero General Public License v3.0 (AGPL-3.0)** æˆæ¬Šç™¼å¸ƒã€‚
è©³æƒ…è«‹åƒé–± [LICENSE](LICENSE) æª”æ¡ˆã€‚

### Dependencies & Acknowledgements

æœ¬å°ˆæ¡ˆåŸºæ–¼å¤šå€‹å„ªç§€çš„é–‹æºå°ˆæ¡ˆæ§‹å»ºï¼Œä½¿ç”¨è€…åœ¨ä½¿ç”¨ã€åˆ†ç™¼æˆ–ä¿®æ”¹æœ¬å°ˆæ¡ˆæ™‚ï¼Œå¿…é ˆåŒæ™‚éµå®ˆä»¥ä¸‹åŸå§‹å°ˆæ¡ˆçš„æˆæ¬Šè¦ç¯„ã€‚æˆ‘å€‘åœ¨æ­¤èª æ‘¯æ„Ÿè¬å„ä½ä½œè€…çš„è²¢ç»ï¼š

#### 1. ğŸ›‘æ ¸å¿ƒé™åˆ¶æ€§çµ„ä»¶ (Restrictive Components)
ä»¥ä¸‹çµ„ä»¶å°æœ¬å°ˆæ¡ˆçš„æˆæ¬Šç¯„åœæœ‰ç›´æ¥å½±éŸ¿ï¼Œè«‹å‹™å¿…æ³¨æ„ï¼š

* **[Ultralytics YOLO](https://docs.ultralytics.com/)** (AGPL-3.0)
    * **å½±éŸ¿**ï¼šç”±æ–¼æœ¬å°ˆæ¡ˆä¾è³´ YOLO é€²è¡Œæ ¸å¿ƒè¨“ç·´èˆ‡æ¨è«–ï¼Œå› æ­¤æ•´é«”å°ˆæ¡ˆç¹¼æ‰¿ AGPL-3.0 è¦ç¯„ã€‚è‹¥æ‚¨å°‡æœ¬å°ˆæ¡ˆï¼ˆæˆ–å…¶ä¿®æ”¹ç‰ˆæœ¬ï¼‰ä½œç‚ºç¶²è·¯æœå‹™æä¾›æˆ–å…¬é–‹ç™¼å¸ƒï¼Œæ‚¨å¿…é ˆå…¬é–‹æ‚¨çš„åŸå§‹ç¢¼ã€‚
* **[ConvNeXt V2](https://github.com/facebookresearch/ConvNeXt-V2)** (Meta Research)
    * **ä»£ç¢¼**ï¼šMIT Licenseã€‚
    * **é è¨“ç·´æ¬Šé‡**ï¼š**CC-BY-NC 4.0 (åƒ…é™éå•†æ¥­ç”¨é€”)**ã€‚
    * **âš ï¸æ³¨æ„**ï¼šè‹¥æ‚¨è¼‰å…¥äº† ConvNeXt V2 çš„å®˜æ–¹é è¨“ç·´æ¬Šé‡ï¼Œæœ¬å°ˆæ¡ˆå°‡è¢«é™åˆ¶ç‚ºåƒ…ä¾›å­¸è¡“ç ”ç©¶æˆ–éå•†æ¥­ç”¨é€”ã€‚
* **[MambaVision](https://github.com/NVlabs/MambaVision)** (NVIDIA)
    * **æˆæ¬Š**ï¼šNVIDIA Source Code License-NC (é€šå¸¸å«æœ‰éå•†æ¥­ç”¨é€”é™åˆ¶ï¼Œè«‹åƒé–±å…¶ Repo ç¢ºèª)ã€‚

#### 2. ğŸ”“å…¶ä»–é–‹æºçµ„ä»¶ (Other Open Source Components)
æœ¬å°ˆæ¡ˆäº¦å¼•ç”¨äº†ä»¥ä¸‹æ¡ç”¨ MITã€Apache-2.0 æˆ– BSD ç­‰å¯¬é¬†æˆæ¬Šçš„å„ªç§€å°ˆæ¡ˆï¼š

* **[SimCC](https://github.com/leeyegy/SimCC)** (MIT)
* **[ConvNeXt V1](https://github.com/facebookresearch/ConvNeXt)** (MIT)
* **[EfficientNet](https://docs.pytorch.org/vision/main/models/efficientnet.html)** (BSD-3-Clause via TorchVision)
* **[InceptionNeXt](https://github.com/sail-sg/inceptionnext)** (Apache-2.0)
* **[HRNet (Bottom-Up)](https://github.com/HRNet/HRNet-Bottom-Up-Pose-Estimation)** (MIT)